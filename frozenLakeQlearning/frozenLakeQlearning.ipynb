{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "frozenLakeQlearning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOC5/gXBIViWbYC0tPie9QJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Isaivargas/machineLearningAgents/blob/master/frozenLake/frozenLakeQlearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0UHQcO03Jcx",
        "colab_type": "text"
      },
      "source": [
        "**The goal of this game is to go from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H).However, the ice is slippery, so you won't always move in the direction you intend (stochastic environment).**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> SFFF       (S: starting point, safe) \n",
        "\n",
        "\n",
        "> FHFH       (F: frozen surface, safe)\n",
        "\n",
        "\n",
        "> FFFH       (H: hole, fall to your doom)\n",
        "\n",
        "\n",
        "\n",
        "> FFFH       (H: hole, fall to your doom)\n",
        "\n",
        "\n",
        "> HFFG       (G: goal, where the frisbee is located)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qksQd-sgJ0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94YkJ64DhjSV",
        "colab_type": "text"
      },
      "source": [
        "The Environment is the Frozen Lake from **Open AI **\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nLx_awnFabe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"FrozenLake-v0\") #This environment produce a 4x4 tiles.\n",
        "#env = gym.make(\"FrozenLake-v0\", desc=None, map_name=None) #This environmnet produce a 8 x 8 tiles\n",
        "#env.reset()  # Returns an initial observation.\n",
        "#env.render() # Render the environment at initial state that is the same as the first observation."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on7D8Nm2hsQO",
        "colab_type": "text"
      },
      "source": [
        "Step #1 \n",
        "Creation of Q Table and initialization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R4CO0tGgOe6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "action_size = env.action_space.n\n",
        "state_size  = env.observation_space.n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJIumjqEuoIb",
        "colab_type": "code",
        "outputId": "74e894cc-90c0-4d42-f937-4e00117ad52f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# Nrows (states) & Mcolumns(actions).\n",
        "qtable = np.zeros((state_size, action_size))\n",
        "print(qtable)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjfgrQGsu2OB",
        "colab_type": "text"
      },
      "source": [
        "Create the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKUDuZ7Pur9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_episodes   = 15000        # Total episodes\n",
        "learning_rate    = 0.6          # Learning rate (How much you accept the new value vs the old value).\n",
        "max_steps        = 99           # Max steps per episode\n",
        "gamma            = 0.99         # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon          = 1.0          # Exploration rate # Set the percent the Agent gonna explore.\n",
        "max_epsilon      = 1.0          # Exploration probability at start\n",
        "min_epsilon      = 0.01         # Minimum exploration probability \n",
        "decay_rate       = 0.001        # Exponential decay rate for exploration prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8LYxEdYvTYT",
        "colab_type": "text"
      },
      "source": [
        "The Q learning algorithm üß†\n",
        "implementation of the Q learning algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIC0xyHHvOvo",
        "colab_type": "code",
        "outputId": "ccf86dbf-2b9a-48d5-8fdc-4a3912265d63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# List of rewards\n",
        "rewards = []\n",
        "\n",
        "# Step 2 For life or until learning is stopped.\n",
        "# Loop of episodes (Trajectories).\n",
        "for episode in range(total_episodes):\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "\n",
        "    # Loop of steps.\n",
        "    for step in range(max_steps):\n",
        "        # Step 3. Choose an action a in the current world state (s)\n",
        "        ## Generate a random number.\n",
        "        exp_exp_tradeoff = random.uniform(0, 1)\n",
        "        \n",
        "        ## If this number > greater than epsilon --> Produce an exploitation (taking the biggest Q value for this state)\n",
        "        if exp_exp_tradeoff > epsilon:\n",
        "            action = np.argmax(qtable[state,:])\n",
        "\n",
        "        # Else doing a random choice --> Produce an exploration.\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "        # qtable[new_state,:] : all the actions we can take from new state\n",
        "        #Learning Process \n",
        "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "        \n",
        "        total_rewards += reward\n",
        "        \n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "        \n",
        "        # If done (if we're dead) : finish episode\n",
        "        if done == True: \n",
        "            break\n",
        "        \n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
        "    rewards.append(total_rewards)\n",
        "\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
        "print(qtable)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score over time: 0.4112\n",
            "[[3.83682771e-01 1.43476236e-01 7.37654001e-02 7.52308320e-02]\n",
            " [7.72831109e-03 2.32112131e-02 1.65793581e-02 1.16574006e-01]\n",
            " [2.50754371e-02 2.05284469e-01 2.04893875e-02 4.95190490e-02]\n",
            " [1.71085464e-02 4.39673930e-02 3.43890396e-02 9.48461806e-02]\n",
            " [4.28220293e-01 8.37185799e-02 2.01327089e-02 2.60791483e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [3.04971005e-03 4.91770654e-05 8.89437502e-02 1.39272241e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [9.75213229e-02 1.40081726e-01 7.22546839e-02 5.52897664e-01]\n",
            " [1.33976449e-02 4.06296906e-01 8.46134296e-02 1.08837124e-01]\n",
            " [6.65461974e-01 2.36130237e-03 2.56463682e-02 6.69168823e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [8.05464650e-02 1.49076653e-01 7.83556940e-01 1.46811007e-01]\n",
            " [4.07139584e-01 9.16606408e-01 2.40297225e-01 6.23947685e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2okbtWPSx5dd",
        "colab_type": "code",
        "outputId": "ae29fa3b-6499-4579-f1ed-2726f48fe302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "env.reset()\n",
        "\n",
        "for episode in range(5):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    print(\"****************************************************\")\n",
        "    print(\"EPISODE \", episode)\n",
        "\n",
        "    for step in range(max_steps):\n",
        "\n",
        "        #env.render() # Render the state of the agent while is playing uncoment to watch it .\n",
        "\n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "        action = np.argmax(qtable[state,:])\n",
        "        \n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        \n",
        "\n",
        "        if done:\n",
        "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
        "            env.render()\n",
        "\n",
        "            # Here we check if the reward is equal to 1 the agent achieve the goal else fell into a hole.\n",
        "            if reward == 1: \n",
        "              print(\"Goal reached üèÜ\")\n",
        "            else:\n",
        "              print(\"Fell into a hole ‚ò†Ô∏è\")\n",
        "            # We print the number of step it took.\n",
        "            print(\"Number of steps\", step)\n",
        "            break\n",
        "        state = new_state\n",
        "env.close()"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "****************************************************\n",
            "EPISODE  0\n",
            "  (Right)\n",
            "SFFF\n",
            "FHF\u001b[41mH\u001b[0m\n",
            "FFFH\n",
            "HFFG\n",
            "Fell into a hole ‚ò†Ô∏è\n",
            "Number of steps 22\n",
            "****************************************************\n",
            "EPISODE  1\n",
            "  (Right)\n",
            "SFFF\n",
            "FHF\u001b[41mH\u001b[0m\n",
            "FFFH\n",
            "HFFG\n",
            "Fell into a hole ‚ò†Ô∏è\n",
            "Number of steps 27\n",
            "****************************************************\n",
            "EPISODE  2\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Goal reached üèÜ\n",
            "Number of steps 16\n",
            "****************************************************\n",
            "EPISODE  3\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Goal reached üèÜ\n",
            "Number of steps 87\n",
            "****************************************************\n",
            "EPISODE  4\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Goal reached üèÜ\n",
            "Number of steps 39\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}