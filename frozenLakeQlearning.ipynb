{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "frozenLakeQlearning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOR6aRxfrr9zsJudKaGuL03",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Isaivargas/machineLearningAgents/blob/master/frozenLakeQlearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0UHQcO03Jcx",
        "colab_type": "text"
      },
      "source": [
        "**The goal of this game is to go from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H).However, the ice is slippery, so you won't always move in the direction you intend (stochastic environment)**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qksQd-sgJ0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94YkJ64DhjSV",
        "colab_type": "text"
      },
      "source": [
        "The Environment is the Frozen Lake from **Open AI **\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OWx6BkAgLo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"FrozenLake-v0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on7D8Nm2hsQO",
        "colab_type": "text"
      },
      "source": [
        "Step #1 \n",
        "Creation of Q Table and initialization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R4CO0tGgOe6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "action_size = env.action_space.n\n",
        "state_size  = env.observation_space.n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJIumjqEuoIb",
        "colab_type": "code",
        "outputId": "e6c63f04-19b2-42ac-a302-5976958eca0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# Nrows (states) & Mcolumns(actions).\n",
        "qtable = np.zeros((state_size, action_size))\n",
        "print(qtable)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjfgrQGsu2OB",
        "colab_type": "text"
      },
      "source": [
        "Create the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKUDuZ7Pur9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_episodes   = 15000        # Total episodes\n",
        "learning_rate    = 0.1          # Learning rate\n",
        "max_steps        = 99           # Max steps per episode\n",
        "gamma            = 0.95         # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon          = 1.0          # Exploration rate\n",
        "max_epsilon      = 1.0          # Exploration probability at start\n",
        "min_epsilon      = 0.01         # Minimum exploration probability \n",
        "decay_rate       = 0.001        # Exponential decay rate for exploration prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8LYxEdYvTYT",
        "colab_type": "text"
      },
      "source": [
        "The Q learning algorithm ðŸ§ \n",
        "implementation of the Q learning algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIC0xyHHvOvo",
        "colab_type": "code",
        "outputId": "d1d68259-ac1e-40d7-cf88-cdad54a28717",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# List of rewards\n",
        "rewards = []\n",
        "\n",
        "# Step 2 For life or until learning is stopped.\n",
        "# Loop of episodes (Trajectories).\n",
        "for episode in range(total_episodes):\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "\n",
        "    # Loop of steps.\n",
        "    for step in range(max_steps):\n",
        "        # Step 3. Choose an action a in the current world state (s)\n",
        "        ## Generate a random number.\n",
        "        exp_exp_tradeoff = random.uniform(0, 1)\n",
        "        \n",
        "        ## If this number > greater than epsilon --> Produce an exploitation (taking the biggest Q value for this state)\n",
        "        if exp_exp_tradeoff > epsilon:\n",
        "            action = np.argmax(qtable[state,:])\n",
        "\n",
        "        # Else doing a random choice --> Produce an exploration.\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "        # qtable[new_state,:] : all the actions we can take from new state\n",
        "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "        \n",
        "        total_rewards += reward\n",
        "        \n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "        \n",
        "        # If done (if we're dead) : finish episode\n",
        "        if done == True: \n",
        "            break\n",
        "        \n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
        "    rewards.append(total_rewards)\n",
        "\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
        "print(qtable)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score over time: 0.5304\n",
            "[[0.21737892 0.16166348 0.17840212 0.17027186]\n",
            " [0.09387525 0.10138598 0.09394584 0.17135135]\n",
            " [0.15023787 0.10673863 0.10321249 0.10865932]\n",
            " [0.05904939 0.05553839 0.06568965 0.10872124]\n",
            " [0.24891298 0.17254694 0.12632492 0.13247031]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.07043108 0.04159124 0.11060563 0.05529311]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.13279136 0.17710833 0.19482411 0.31036498]\n",
            " [0.14869659 0.41772571 0.30244281 0.20835614]\n",
            " [0.38339312 0.23482281 0.2365729  0.17984912]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.20713168 0.336669   0.5565108  0.3767499 ]\n",
            " [0.57298619 0.74720964 0.55940739 0.57545256]\n",
            " [0.         0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2okbtWPSx5dd",
        "colab_type": "code",
        "outputId": "1d0881cb-6256-4da1-8008-e21332a222ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "env.reset()\n",
        "\n",
        "for episode in range(5):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    print(\"****************************************************\")\n",
        "    print(\"EPISODE \", episode)\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        \n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "        action = np.argmax(qtable[state,:])\n",
        "        \n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        if done:\n",
        "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
        "            env.render()\n",
        "            \n",
        "            # We print the number of step it took.\n",
        "            print(\"Number of steps\", step)\n",
        "            break\n",
        "        state = new_state\n",
        "env.close()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "****************************************************\n",
            "EPISODE  0\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 8\n",
            "****************************************************\n",
            "EPISODE  1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 42\n",
            "****************************************************\n",
            "EPISODE  2\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 80\n",
            "****************************************************\n",
            "EPISODE  3\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 62\n",
            "****************************************************\n",
            "EPISODE  4\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 61\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}